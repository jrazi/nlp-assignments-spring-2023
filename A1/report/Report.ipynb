{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5f4631",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "    <img src='./assets/images/besm.png' width='150px'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c158f922-2a45-4d1e-8975-2c7ff804f0d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .jp-MarkdownOutput {\n",
       "        direction: rtl;\n",
       "        text-align: right;\n",
       "        font-family: 'Vazir', 'B Nazanin', 'B Lotus', 'Arial', 'Verdana'\n",
       "    }\n",
       "    h4 {\n",
       "        padding: 0px;\n",
       "        margin: 0px;\n",
       "    }\n",
       "    div, span {\n",
       "        font-size: 14px;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    .jp-MarkdownOutput {\n",
    "        direction: rtl;\n",
    "        text-align: right;\n",
    "        font-family: 'Vazir', 'B Nazanin', 'B Lotus', 'Arial', 'Verdana'\n",
    "    }\n",
    "    h4 {\n",
    "        padding: 0px;\n",
    "        margin: 0px;\n",
    "    }\n",
    "    div, span {\n",
    "        font-size: 14px;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989770aa-2aab-4ceb-806b-7f4c36a95718",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3><center>تمرین نخست درس «پردازش زبان‌های طبیعی»</center></h3>\n",
    "<h4><center> جواد راضی (۴۰۱۲۰۴۳۵۴) </center></h4>\n",
    "<h4><center> ترم ۴۰۱۲ </center></h4>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78b015",
   "metadata": {},
   "source": [
    "\n",
    "<h2>\n",
    "    متن انتخابی\n",
    "</h2>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f459070",
   "metadata": {},
   "source": [
    "<span>\n",
    "    در این تمرین، کامنت‌های زیر چند شعر مشهور در وب‌سایت گنجور استخراج شده، و بر روی آن‌ها یکسری پردازش اعمال می‌شود. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658ea75-49ad-4a92-8c76-64d894553531",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    کاوش متن و استخراج کامنت‌های سایت\n",
    "</h2>\n",
    "<div>\n",
    "    <span>\n",
    "        برای crawling، از ابزار scrapy در این تمرین استفاده شده‌است.\n",
    "    </span>\n",
    "    <span>\n",
    "        scrapy، یک کتاب‌خانه پایتون است که به واسطه آن می‌توان اطلاعات مورد نظر، نظیر متون،‌تصاویر، و محتوای بخش‌های خاصی از سایت را از آن استخراج نمود. \n",
    "    </span>\n",
    "    <br/>\n",
    "    <span>\n",
    "        اسکریپت پایتون زیر، یک کامنت‌های مربوط به چند شعر انتخابی از خیام، حافظ، و پروین اعتصامی را استخراج می‌نماید. کامنت‌های کراول شده، ساختاری nested دارند؛ به عبارت دیگر، برای هر شعر، کامنت‌‌ها خود می‌توانند شامل کامنت‌های پاسخ باشند که در قالب یک آبجکت nested ذخیره می‌شود.\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15fec39-84fa-45b0-9c4f-8f05b2709a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import scrapy \n",
    "except: \n",
    "    !pip3 install scrapy\n",
    "try:\n",
    "    import crochet\n",
    "except:\n",
    "    !pip3 install crochet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f9caea-98f9-4e97-af30-c233509691dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape webpage\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "# text cleaning\n",
    "import re\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96685a50-0a1d-47e9-9648-276d74041aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanjoorSpider(scrapy.Spider):\n",
    "    name = \"GanjoorSpider\"\n",
    "    start_urls = [\n",
    "        'https://ganjoor.net/khayyam/robaee/sh41',\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            # '__main__.ExtractFirstLine': 1\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'ganjoor.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"parse data from urls\"\"\"\n",
    "        for comment in response.css('#comments-block > .ganjoor-comment > blockquote > p:text'):\n",
    "            yield {'comment': comment.extract()}\n",
    "\n",
    "\n",
    "class ExtractFirstLine(object):\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"text processing\"\"\"\n",
    "        lines = dict(item)[\"comment\"].splitlines()\n",
    "        first_line = self.__remove_html_tags__(lines[0])\n",
    "\n",
    "        return {'quote': first_line}\n",
    "\n",
    "    def __remove_html_tags__(self, text):\n",
    "        html_tags = re.compile('<.*?>')\n",
    "        return re.sub(html_tags, '', text)\n",
    "\n",
    "\n",
    "@wait_for(10)\n",
    "def run_spider():\n",
    "    crawler = CrawlerRunner()\n",
    "    d = crawler.crawl(GanjoorSpider)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0513a83-b59b-4639-9dd0-10c32e303f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43147160-e7d4-4356-91d2-ede946315659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scrapy\n",
    "#\n",
    "# class GanjoorSpider(scrapy.Spider):\n",
    "#     name = \"ganjoor_comments\"\n",
    "#     start_urls = [\n",
    "#         \"https://ganjoor.net/khayyam/robaee/sh41\"\n",
    "#     ]\n",
    "#\n",
    "#     def parse(self, response):\n",
    "#         for comment in response.css('.comment'):\n",
    "#             yield {\n",
    "#                 'author': comment.css('.author::text').get(),\n",
    "#                 'text': comment.css('.text::text').get(),\n",
    "#                 'date': comment.css('.date::text').get(),\n",
    "#             }\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b1db9",
   "metadata": {},
   "source": [
    "# Parsi-IO \n",
    "\n",
    "### https://github.com/language-ml/parsi.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdba52",
   "metadata": {},
   "source": [
    "تلاش گروهی متن‌باز برای استخراج اطلاعات فارسی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f59687",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parsi_io'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16300\\3057682687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mparsi_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress_extractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAddressExtraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAddressExtraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mextractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'آدرس خیابان شهیدبهشتی می‌باشد و برای اطلاعات بیشتر به page.com مراجعه فرمایید'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'parsi_io'"
     ]
    }
   ],
   "source": [
    "from parsi_io.modules.address_extractions import AddressExtraction\n",
    "extractor = AddressExtraction()\n",
    "extractor.run('آدرس خیابان شهیدبهشتی می‌باشد و برای اطلاعات بیشتر به page.com مراجعه فرمایید')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsi_io.modules.number_extractor import NumberExtractor\n",
    "extractor = NumberExtractor()\n",
    "extractor.run(\" یک صد و بیست میلیون تومان هزینه داشت\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881adb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsi_io.modules.time_extractions import TimeExtraction\n",
    "extractor = TimeExtraction()\n",
    "extractor.run(\"روز  ۷ اسفند کتابخانه پانزده خرداد می آیم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20495294",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نمونه‌ای از متن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "#tqdm.tqdm_notebook()\n",
    "    \n",
    "# Path of poems\n",
    "poem_path = \"../../exploring-datasets/literature/iranian\"\n",
    "HAFEZ_end=8384\n",
    "SAADI_start=9384\n",
    "mesra_collection = [x.strip() for x in tqdm.notebook.tqdm(codecs.open(F'{poem_path}/qazals/all_qazals_mesra.txt','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b864bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesra_collection[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed5ba9",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نرمالایز کردن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75b896",
   "metadata": {},
   "source": [
    "# Hazm \n",
    "\n",
    "### https://github.com/roshan-research/hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3f0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "hazm_normalizer = Normalizer()\n",
    "\n",
    "mesra_normalized_hazm = [hazm_normalizer.normalize(x) for x in tqdm.notebook.tqdm(mesra_collection)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f068c4",
   "metadata": {},
   "source": [
    "# Dadmatools \n",
    "\n",
    "### https://github.com/Dadmatech/DadmaTools\n",
    "### https://aclanthology.org/2022.naacl-demo.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadmatools.models.normalizer import Normalizer\n",
    "\n",
    "dadma_normalizer = Normalizer(\n",
    "    full_cleaning=True,\n",
    "    unify_chars=True,\n",
    "    refine_punc_spacing=True,\n",
    "    remove_extra_space=True,\n",
    "    remove_puncs=False,\n",
    "    remove_html=False,\n",
    "    remove_stop_word=False,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=\"<NUM>\",\n",
    "    replace_url_with=\"\",\n",
    "    replace_mobile_number_with=None,\n",
    "    replace_emoji_with=None,\n",
    "    replace_home_number_with=None,\n",
    ")\n",
    "\n",
    "mesra_normalized_dadma = [dadma_normalizer.normalize(x) for x in tqdm.notebook.tqdm(mesra_collection)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6221c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mesra = random.sample(range(0,len(mesra_collection)),10)\n",
    "examples = [\"روحُ القدس2 \",\"می توانم\"]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'original_text':[mesra_collection[idx] for idx in random_mesra]+examples,\n",
    "                   'hazm_normalized':[mesra_normalized_hazm[idx] for idx in random_mesra]\n",
    "                       +[hazm_normalizer.normalize(e) for e in examples],\n",
    "                   'dadma_normalized':[mesra_normalized_dadma[idx] for idx in random_mesra]\n",
    "                       +[dadma_normalizer.normalize(e) for e in examples]\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61c9cb",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § جمله‌بندی  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesra_sentences = [sent_tokenize(x) for x in tqdm.notebook.tqdm(mesra_normalized_hazm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mesra = random.sample(range(0,len(mesra_collection)),10)\n",
    "examples = [\"علی به خانه رفت . آنجا دو روز ماند. \",\"ای.آی مخفف هوش مصنوعی است\"]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'original_text':[mesra_collection[idx] for idx in random_mesra]+examples,\n",
    "                   'hazm_text':['---'.join(mesra_sentences[idx]) for idx in random_mesra]\n",
    "                       +['---'.join(sent_tokenize(e)) for e in examples],\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44124d",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § توکنایزیشن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99815c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesra_tokens = [[word_tokenize(sent) for sent in sents] for sents in tqdm.notebook.tqdm(mesra_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21090a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mesra = random.sample(range(0,len(mesra_collection)),10)\n",
    "examples = [\" او خوش‌برخورد است\"]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'original_text':[mesra_collection[idx] for idx in random_mesra]+examples,\n",
    "                   'hazm_text':[mesra_tokens[idx] for idx in random_mesra]\n",
    "                       +[word_tokenize(hazm_normalizer.normalize(e)) for e in examples],\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f1e9d",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § تحلیل بسامد  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "all_tokens = list(itertools.chain(*itertools.chain(*mesra_tokens)))\n",
    "hafez_tokens = list(itertools.chain(*itertools.chain(*mesra_tokens[0:HAFEZ_end])))\n",
    "saadi_tokens = list(itertools.chain(*itertools.chain(*mesra_tokens[SAADI_start::])))\n",
    "\n",
    "dataframe = {}\n",
    "\n",
    "for opt in ['hafez', 'saadi', 'all']:\n",
    "     dataframe[opt] = FreqDist(eval(F\"{opt}_tokens\")).most_common(25)\n",
    "\n",
    "freq_analysis = pd.DataFrame(dataframe)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e63104",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('%-16s' % 'Number of words', '%-16s' % len(all_tokens))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(all_tokens)))\n",
    "avg=np.sum([len(word) for word in all_tokens])/len(all_tokens)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % all_tokens[np.argmax([len(word) for word in all_tokens])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda235f",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § حذف stop-word  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persian Stopwords\n",
    "# https://github.com/sobhe/hazm/blob/master/hazm/data/stopwords.dat\n",
    "stop_path = '../../resources/persian_stopwords.txt'\n",
    "stopwords = [hazm_normalizer.normalize(x.strip()) for x in codecs.open(stop_path,'r','utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_path_poems = '../../resources/persian_stopwords-poems.txt'\n",
    "stopwords = stopwords + [hazm_normalizer.normalize(x.strip()) for x in codecs.open(stop_path_poems,'r','utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb99f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_nonstop = [t for t in tqdm.tqdm(all_tokens) if t not in stopwords]\n",
    "hafez_tokens_nonstop = [t for t in tqdm.tqdm(hafez_tokens) if t not in stopwords]\n",
    "saadi_tokens_nonstop = [t for t in tqdm.tqdm(saadi_tokens) if t not in stopwords]\n",
    "\n",
    "dataframe_nonstop = {}\n",
    "\n",
    "for opt in ['hafez', 'saadi', 'all']:\n",
    "     dataframe_nonstop[opt] = FreqDist(eval(F\"{opt}_tokens_nonstop\")).most_common(50)\n",
    "\n",
    "freq_analysis_nonstop = pd.DataFrame(dataframe_nonstop)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a9b4e",
   "metadata": {},
   "source": [
    "<h4 style='direction:rtl;'> برای داده خود لیست stop-word ها را بهبود بدهید </h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7448e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_analysis_nonstop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a411f7",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از lemmatization, stemming  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def get_lemma_set(tok, opt=1):\n",
    "    if opt ==1:\n",
    "        return stemmer.stem(tok)\n",
    "    if opt ==2:\n",
    "        return lemmatizer.lemmatize(tok)\n",
    "    if opt ==3:\n",
    "        # write your own\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<NUM> <DATE> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9691e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = 2\n",
    "\n",
    "all_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(all_tokens_nonstop)]\n",
    "hafez_tokens_nonstop_lemstem = [get_lemma_set(t, opt) for t in tqdm.tqdm(hafez_tokens_nonstop)]\n",
    "saadi_tokens_nonstop_lemstem = [get_lemma_set(t, opt) for t in tqdm.tqdm(saadi_tokens_nonstop)]\n",
    "\n",
    "dataframe_nonstop_lemstem = {}\n",
    "\n",
    "for opt in ['hafez', 'saadi', 'all']:\n",
    "     dataframe_nonstop_lemstem[opt] = FreqDist(eval(F\"{opt}_tokens_nonstop_lemstem\")).most_common(50)\n",
    "\n",
    "freq_analysis_nonstop_lemstem = pd.DataFrame(dataframe_nonstop_lemstem)   \n",
    "freq_analysis_nonstop_lemstem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbb2c3",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از POS-tags  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = POSTagger(model='../../resources/postagger.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f63b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dataframe_nonstop_lemstem_advjj = {}\n",
    "\n",
    "for opt in tqdm.tqdm(['hafez', 'saadi', 'all']):\n",
    "    selected = []\n",
    "    for x,y in FreqDist(eval(F\"{opt}_tokens_nonstop_lemstem\")).most_common(1000):\n",
    "        pos = tagger.tag([x])[0][1]\n",
    "        if pos in ['V']:\n",
    "            selected.append((x,y)) \n",
    "    dataframe_nonstop_lemstem_advjj[opt]= copy.deepcopy(selected[0:40])\n",
    "\n",
    "dataframe_nonstop_lemstem_advjj = pd.DataFrame(dataframe_nonstop_lemstem_advjj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the code to get the POS's in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25604993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_nonstop_lemstem_advjj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d56f7",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § دیدن سیاق  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7eefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hafez_text = nltk.Text(hafez_tokens_nonstop_lemstem)\n",
    "hafez_text.concordance('آب')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e3418",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § چانکینگ - عبارت یابی  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a74d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in mesra_tokens[0:10]:\n",
    "    print(tagger.tag(sent[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b08fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.tag(hazm_normalizer.normalize(\"حافظ خلوت نشین\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vasf_ext(sentence):\n",
    "    grammar = r\"\"\"\n",
    "      NVASF: {<N|Ne><AJ>}\n",
    "      GHEIDV: {<ADV><V>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return (cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b692790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.tag(mesra_tokens[100][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vasf_ext(tagger.tag(mesra_tokens[100][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ebdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosufs = []\n",
    "gheids = []\n",
    "for sentence in tqdm.tqdm(mesra_tokens[0:HAFEZ_end]):\n",
    "    tree=vasf_ext(tagger.tag(sentence[0]))\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NVASF':\n",
    "            mosufs.append(subtree.leaves())\n",
    "        if subtree.label() == 'GHEIDV':\n",
    "            gheids.append(subtree.leaves())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befe064",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosuf_texts = [' '.join([x[0],y[0]]) for x,y in mosufs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96585971",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosuf_texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2945be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "results['hafez'] = nltk.FreqDist(mosuf_texts).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd698ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosufs = []\n",
    "gheids = []\n",
    "for sentence in tqdm.tqdm(mesra_tokens[SAADI_start::]):\n",
    "    tree=vasf_ext(tagger.tag(sentence[0]))\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'NVASF':\n",
    "            mosufs.append(subtree.leaves())\n",
    "        if subtree.label() == 'GHEIDV':\n",
    "            gheids.append(subtree.leaves())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d2f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosuf_texts = [' '.join([x[0],y[0]]) for x,y in mosufs]\n",
    "mosuf_texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['saadi'] = nltk.FreqDist(mosuf_texts).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ade055",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b352a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hafez = ' '.join([' '.join(s[0]) for s in mesra_tokens[0:8384]])\n",
    "#saadi = ' '.join([' '.join(s[0]) for s in mesra_tokens[9384::]])\n",
    "hafez= ' '.join(hafez_tokens_nonstop)\n",
    "saadi= ' '.join(saadi_tokens_nonstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(hafez)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92086aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(saadi)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae34c3",
   "metadata": {},
   "source": [
    "# Dadma Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dadmatools.pipeline.language as language\n",
    "\n",
    "# here lemmatizer and pos tagger will be loaded\n",
    "# as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "pips = 'tok,lem,pos,dep,ner,chunk,cons,kasreh' \n",
    "nlp = language.Pipeline(pips)\n",
    "\n",
    "# you can see the pipeline with this code\n",
    "print(nlp.analyze_pipes(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('مسابقات کشتی در کشور ایران برگزار خواهد شد')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5342e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278be31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.constituency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee1313",
   "metadata": {},
   "source": [
    "# Weighted Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U strsimpy\n",
    "import pandas as pd\n",
    "from strsimpy.weighted_levenshtein import WeightedLevenshtein\n",
    "\n",
    "\n",
    "def insertion_cost(char):\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def deletion_cost(char):\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def substitution_cost(char_a, char_b):\n",
    "    return 2.0\n",
    "\n",
    "weighted_levenshtein = WeightedLevenshtein(\n",
    "    substitution_cost_fn=substitution_cost,\n",
    "    insertion_cost_fn=insertion_cost,\n",
    "    deletion_cost_fn=deletion_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weighted_levenshtein.distance('لانه', 'لثه'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c47756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weighted_levenshtein.distance('لانه', 'لاله'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weighted_levenshtein.distance('لانه', 'لستم'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weighted_levenshtein.distance('لثه', 'لستم'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f7be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
