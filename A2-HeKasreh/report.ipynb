{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mYE-vgHGxwL"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3><center>تمرین دوم درس پردازش زبان‌های طبیعی</center></h3>\n",
        "<h4><center>چالش درست‌کردن هه کسره (گروه چهار)</center></h4>\n",
        "<table width='100%' style=\"border: none;\">\n",
        "    <tr style=\"border: none; text-align: center;\">\n",
        "        <td style=\"border: none;\"><h5>علیرضا بلال</h5></td>\n",
        "        <td style=\"border: none;\"><h5>زهرا رجالی</h5></td>\n",
        "        <td style=\"border: none;\"><h5>جواد راضی</h5></td>\n",
        "    </tr>\n",
        "</table>\n",
        "<h5 style=\"font-size: 16px;\"><center> ترم ۴۰۱۲ </center></h5>\n",
        "<br/>\n",
        "<hr/>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RAmvwTvGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "#    **خطای هه کسره**\n",
        "در نوشتار فارسی، خطای \"ه‌کسره\" هنگامی به وجود می‌آید  که نشان کسره به درستی استفاده نشود.\n",
        "با اینکه صدای \"e\" در زبان فارسی دارای چندین نوع تکواژ است، اما برای نمایش آن در نوشتار دو نماد تکواژی وجود دارد. در مواقعی که به جای کسره (ـــِ) از \"ه/ـه\" استفاده شود یا برعکس، خطای گرامری هکسره به وجود می‌آید. در این تمرین، سرویسی را پیاده‌سازی کرده‌ایم که با دریافت یک متن فارسی، خطاهای «ه‌هکسره» آن را تشخیص داده و متن تصحیح شده را در پاسخ بر می‌گرداند. در ادامه گزارش، جزئیات پیاده‌سازی تمرین، و شیوه بکاررفته برای تشخیص خطای ه‌کسره شرح داده شده‌است. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzBffAvSGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "### **نصب پکیج‌ها و ابزارهای مورد نیاز**\n",
        "\n",
        "کتاب‌خانه‌های اصلی مورد استفاده در این تمرین، کتاب‌خانه‌های هضم و دادماتولز بوده‌اند. کتاب‌خانه هضم برای POS Tagging و دادماتولز برای بررسی شباهت کلمات استفاده شده‌است. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ECGenxGxwO",
        "outputId": "dbfc1b33-2dae-4b2a-f8d5-dbe7cdc55f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (67.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395726 sha256=2f405b44249c4a8ca2571748b609bbfae90a789703810ba8a3ddd43139a14839\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import hazm\n",
        "except:\n",
        "    %pip install hazm\n",
        "    \n",
        "try:\n",
        "    import dadmatools\n",
        "except:\n",
        "    %pip install dadmatools\n",
        "\n",
        "try:\n",
        "    import fasttext\n",
        "except:\n",
        "    %pip install fasttext\n",
        "\n",
        "try: \n",
        "    import wapiti\n",
        "except: \n",
        "    %pip install wapiti\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yku9-tEzGxwP"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "## **هم‌خوان‌سازی کد با کتاب‌خانه parsi-io**\n",
        "در پیاده‌سازی کد این تمرین،‌تلاش شده‌ که شیوه پیاده‌سازی سرویس، با الگوی پیاده‌سازی ابزار کتاب‌خانه parsi-io همگام باشد. \n",
        "`HeKasraExtractor`، کلاس\n",
        " اصلی این سرویس است که تلاش شده با چارچوبی متناسب با کتاب‌خانه یادشده توسعه داده شود. البته به علت رسیدن به ددلاین آپلود تمرین، فرصت برای پول ریکوئست به ریپازیتوری این کتاب‌خانه مهیا نشد. با تکمیل رعایت گایدلاین‌ها و قواعد کانتریبیوشن به ریپازیتوری پروژه یادشده، تلاش خواهیم کرد در آینده نزدیک به ریپازیتوری پارسی‌آی‌او پول‌ریکوئست دهیم. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJeC323GxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "# **یافتن و تصحیح ایرادات هه کسره**\n",
        "<br>\n",
        "\n",
        "در نوشتار فارسی، خطای ه‌کسره با الگوهای خاصی رخ می‌دهد. به طور کلی،‌این الگوها تا حد زیادی قاعده‌مند می‌باشند و با وجود استثنائات، می‌توان خطاهای «ه‌کسره» را به چند دسته خاص تقسیم کرد.  \n",
        "در این [بلاگ‌پست](https://blog.irandargah.com/غلط‌های-نگارشی-و-املایی،-قاتل-اعتبار‎/) \n",
        "به دسته‌بندی‌های مختلف خطای هکسره در زبان فارسی اشاره مختصر و مفیدی شده‌است. در این تمرین نیز پیاده‌سازی تشخیص هکسره را بر مبنای منابع از این دست انجام داده‌ایم. در ادامه به طور مختصر الگوهای رایج این خطای املایی شرح داده می‌شوند. \n",
        "<h4>\n",
        "    بیان روش کلی\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEZx7BEjGxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3>\n",
        "    پیاده‌سازی کلاس HeKasraExtractor\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "XUJIKyhzGxwQ"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class HeKasraCorrection: \n",
        "    def __init__(self, processed_text):\n",
        "        self.processed_text = processed_text\n",
        "        self.corrections = {\n",
        "            'correct': processed_text['raw_text'],\n",
        "        }\n",
        "        self.correction_judgements = defaultdict(list)\n",
        "    \n",
        "    def vote_for_correction(self, invalid_token, corrected_token, str_index, order=10):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': invalid_token,\n",
        "            'corrected_token': corrected_token,\n",
        "            'str_index': str_index,\n",
        "            'order': order,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        "    \n",
        "    def veto_correction(self, already_correct_token, str_index):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': already_correct_token,\n",
        "            'corrected_token': already_correct_token,\n",
        "            'str_index': str_index,\n",
        "            'order': 0,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        " \n",
        "    def apply_correction_judgements(self, token, str_index):\n",
        "        # print('applying', token, str_index, self.correction_judgements)\n",
        "        judgements = self.correction_judgements[str_index]\n",
        "        # print('and now judgements', self.correction_judgements, self.correction_judgements.keys(), token, str_index)\n",
        "        if len(judgements) == 0:\n",
        "            return\n",
        "        \n",
        "        # print('judgements for token', token, str_index, judgements)\n",
        "        sorted_judgements = sorted(judgements, key=lambda x: x['order'])\n",
        "        prioritized_correction = sorted_judgements[0]\n",
        "        corrected_form = self.corrections['correct'][:str_index] + prioritized_correction['corrected_token'] + self.corrections['correct'][str_index+len(prioritized_correction['invalid_token']):]\n",
        "        self.corrections['correct'] = corrected_form\n",
        "        if token != prioritized_correction['corrected_token']:\n",
        "            self.corrections[prioritized_correction['invalid_token']] = [int(str_index), int(str_index)+len(prioritized_correction['invalid_token'])] \n",
        "              \n",
        "    def finalize(self):\n",
        "        for str_index in self.correction_judgements.copy().keys():\n",
        "            self.apply_correction_judgements(self.correction_judgements['invalid_token'], str_index)\n",
        "        if self.corrections['correct'] == self.processed_text['raw_text']:\n",
        "          self.corrections = {}\n",
        "        return {\n",
        "            **self.processed_text,\n",
        "            'correction': self.corrections,\n",
        "        }       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some downloading, so separate the cell\n",
        "embeddings = get_embedding('glove-wiki')"
      ],
      "metadata": {
        "id": "7nUBayxdKNFe"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "PK3TWkIUGxwQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from hazm import WordTokenizer, POSTagger, Normalizer, Lemmatizer\n",
        "from dadmatools.embeddings import get_embedding\n",
        "\n",
        "normalizer = Normalizer()\n",
        "tokenizer = WordTokenizer()\n",
        "tagger = POSTagger(model=\"model/postagger.model\")\n",
        "\n",
        "class HeKasraExtractor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        normalized_text = normalizer.normalize(text)\n",
        "        tokens = tokenizer.tokenize(normalized_text)\n",
        "        tagged_tokens = tagger.tag(tokens)\n",
        "\n",
        "        return {\n",
        "            'raw_text': text,\n",
        "            'normalized_text': normalized_text,\n",
        "            'tokens': tokens,\n",
        "            'pos_tags': tagged_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "    def vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "\n",
        "        # he_kasra_pattern = re.compile(r\"^[^ه]+[ِ]\\b\")\n",
        "\n",
        "        pos_pairs = zip(processed_text['pos_tags'][:-1], processed_text['pos_tags'][1:])\n",
        "\n",
        "        for ppair in pos_pairs:\n",
        "          p1, p2 = ppair\n",
        "          token_1, tag_1 = p1\n",
        "          token_2, tag_2 = p2\n",
        "          first_token_roles = ('N', 'Ne', 'PRO', 'AJ')\n",
        "          second_token_roles = ('N', 'Ne', 'AJ', 'PRO')\n",
        "          if tag_1 in first_token_roles and tag_2 in second_token_roles:\n",
        "              if re.match(he_kasra_pattern, token_1):\n",
        "                he_kasra_correction.vote_for_correction(token_1, token_1[:-1], processed_text['raw_text'].index(token_1))\n",
        "\n",
        "    def check_word_contains_he(self, word):\n",
        "        \n",
        "        normalized_word = normalizer.normalize(word)\n",
        "        if not normalized_word.endswith('ه'):\n",
        "            pass\n",
        "        \n",
        "        word_without_he = normalized_word[:-1]\n",
        "        try:\n",
        "          similarity = embeddings.similarity(word_without_he, normalized_word)\n",
        "          # Some arbitrary threshold, cause we're not a bunch of data scientists doing data science here. \n",
        "          return similarity > 0.8\n",
        "\n",
        "        except KeyError:\n",
        "          return False\n",
        "    \n",
        "    def veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "        for token, tag in processed_text['pos_tags']:\n",
        "            contains_he = self.check_word_contains_he(token)\n",
        "            if contains_he:\n",
        "                he_kasra_correction.veto_correction(token, processed_text['raw_text'].index(token))\n",
        "                                \n",
        "                                \n",
        "    def run(self, input_sentence):\n",
        "        prep_text = self.preprocess(input_sentence)\n",
        "        he_kasra_correction = HeKasraCorrection(prep_text)\n",
        "        \n",
        "        pipe = [\n",
        "            self.vote_n_adj_he_kasra, \n",
        "            self.veto_if_word_he_part_of_word,\n",
        "        ]\n",
        "        for func in pipe:\n",
        "            func(he_kasra_correction, prep_text)\n",
        "            \n",
        "        result = he_kasra_correction.finalize()\n",
        "        # print(result)\n",
        "        return result['correction']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgM5R2UYGxwR",
        "outputId": "fafba004-cd29-4fc5-c00b-fececa553381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('correct', 'کوروش کبیر'), ('کوروشه', [0, 6])])\n",
            "dict_items([])\n",
            "dict_items([('correct', 'حال من خوبه'), ('حاله', [0, 4])])\n",
            "dict_items([])\n",
            "dict_items([('correct', 'این دختر دیوانه'), ('دختره', [4, 9])])\n",
            "dict_items([])\n",
            "dict_items([('correct', 'گل زیبایی را تقدیم کردم'), ('گله', [0, 3])])\n"
          ]
        }
      ],
      "source": [
        "hkasra_extractor = HeKasraExtractor()\n",
        "input_samples = ['کوروشه کبیر', 'حال من خوب است.', 'حاله من خوبه', 'من اگه کتابه تو رو داشتم', 'این دختره دیوانه', 'گل زیبا', 'گله زیبایی را تقدیم کردم']\n",
        "\n",
        "for sample in input_samples:\n",
        "  res = hkasra_extractor.run(sample)\n",
        "  print(res.items())\n",
        "  # print(hkasra_extractor.run(sample))\n",
        "    # print(hkasra_extractor.preprocess(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UESv4uYkGxwR"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3>\n",
        "    ارزیابی ماژول\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VsvYuXeGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h4>\n",
        "    خطای ه اضافه بجای نقش‌نمای اضافه\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJ0NRj3GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHMbFb-tGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h4>\n",
        "    خطای عدم جای‌گذاری هکسره\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufkaPbX5GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPLnVLaGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3>\n",
        "    تست عملکرد ماژول با تست‌های اتوماتیک\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBEJYEIIGxwT"
      },
      "outputs": [],
      "source": [
        "class HeHasraTests:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXxw3ofQGxwQ"
      },
      "outputs": [],
      "source": [
        "vowels = {'ا', 'و', 'ی', 'آ', 'اُ', 'اِ', 'اَ'}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}