{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mYE-vgHGxwL"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3><center>تمرین دوم درس پردازش زبان‌های طبیعی</center></h3>\n",
        "<h4><center>چالش درست‌کردن هه کسره (گروه چهار)</center></h4>\n",
        "<table width='100%' style=\"border: none;\">\n",
        "    <tr style=\"border: none; text-align: center;\">\n",
        "        <td style=\"border: none;\"><h5>علیرضا بلال</h5></td>\n",
        "        <td style=\"border: none;\"><h5>زهرا رجالی</h5></td>\n",
        "        <td style=\"border: none;\"><h5>جواد راضی</h5></td>\n",
        "    </tr>\n",
        "</table>\n",
        "<h5 style=\"font-size: 16px;\"><center> ترم ۴۰۱۲ </center></h5>\n",
        "<br/>\n",
        "<hr/>\n",
        "<br/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar', 'B Lotus', 'Calibri'\" size=3><div dir='rtl' align='justify'>\n",
        "<b>\n",
        "    فایل ژوپیتر این تمرین در کولب توسعه داده و تست شده‌است. ابزارهایی نظیر دادماتولز، به تجربه ما، در بعضی از محیط‌های محلی به خاطر یک سری تداخلات با نسخه پکیج‌ها در نصب به مشکل می‌خورند. اما این فایل هم در محیط کولب، هم با ایمیج داکر jupyter/datascience-notebook تست شده‌است و همه قطعه‌کدها خروجی مورد انتظار را می‌دهند. اگه در بازتولید خروجی بعضی سل‌ها مشکلی وجود داشت، ممنون می‌شویم در صورت امکان به ما اطلاع دهید تا فایل را در محیطی که قابل اجرا است، اجرا نموده و خروجی را نمایش دهیم. . \n",
        "</b>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6RAmvwTvGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "#    **خطای هه کسره**\n",
        "در نوشتار فارسی، خطای \"ه‌کسره\" هنگامی به وجود می‌آید  که نشان کسره به درستی استفاده نشود.\n",
        "با اینکه صدای \"e\" در زبان فارسی دارای چندین نوع تکواژ است، اما برای نمایش آن در نوشتار دو نماد تکواژی وجود دارد. در مواقعی که به جای کسره (ـــِ) از \"ه/ـه\" استفاده شود یا برعکس، خطای گرامری هکسره به وجود می‌آید. در این تمرین، سرویسی را پیاده‌سازی کرده‌ایم که با دریافت یک متن فارسی، خطاهای «ه‌هکسره» آن را تشخیص داده و متن تصحیح شده را در پاسخ بر می‌گرداند. در ادامه گزارش، جزئیات پیاده‌سازی تمرین، و شیوه بکاررفته برای تشخیص خطای ه‌کسره شرح داده شده‌است. \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CzBffAvSGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "### **نصب پکیج‌ها و ابزارهای مورد نیاز**\n",
        "\n",
        "کتاب‌خانه‌های اصلی مورد استفاده در این تمرین، کتاب‌خانه‌های هضم و دادماتولز بوده‌اند. کتاب‌خانه هضم برای POS Tagging و دادماتولز برای بررسی شباهت کلمات استفاده شده‌است. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ECGenxGxwO",
        "outputId": "dbfc1b33-2dae-4b2a-f8d5-dbe7cdc55f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dadmatools\n",
            "  Using cached dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
            "Collecting supar==1.1.2\n",
            "  Using cached supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "Collecting bpemb>=0.3.3\n",
            "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting pyconll>=3.1.0\n",
            "  Using cached pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting Deprecated==1.2.6\n",
            "  Using cached Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Collecting py7zr>=0.17.2\n",
            "  Using cached py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "Collecting torch>=1.7.1\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Collecting conllu\n",
            "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting folium>=0.2.1\n",
            "  Using cached folium-0.14.0-py2.py3-none-any.whl (102 kB)\n",
            "Collecting segtok>=1.5.7\n",
            "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "  Using cached pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "Collecting tabulate>=0.8.6\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from dadmatools) (3.3)\n",
            "Collecting sklearn>=0.0\n",
            "  Using cached sklearn-0.0.post4.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting hyperopt>=0.2.5\n",
            "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting gdown>=4.3.1\n",
            "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Collecting gensim>=3.6.0\n",
            "  Using cached gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
            "Collecting spacy>=3.0.0\n",
            "  Using cached spacy-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Collecting transformers>=4.9.1\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "Requirement already satisfied: h5py>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dadmatools) (3.8.0)\n",
            "Collecting NERDA\n",
            "  Using cached NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting html2text\n",
            "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting wrapt<2,>=1.10\n",
            "  Using cached wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Collecting stanza\n",
            "  Using cached stanza-1.5.0-py3-none-any.whl (802 kB)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (1.23.5)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (2.28.2)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.10/site-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n",
            "Collecting branca>=0.6.0\n",
            "  Using cached branca-0.6.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.3.1->dadmatools) (4.12.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "Collecting py4j\n",
            "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.10/site-packages (from hyperopt>=0.2.5->dadmatools) (3.1)\n",
            "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Collecting future\n",
            "  Using cached future-0.18.3.tar.gz (840 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4\n",
            "  Using cached pyzstd-0.15.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Using cached pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Collecting multivolumefile>=0.2.3\n",
            "  Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Using cached pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Collecting texttable\n",
            "  Using cached texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr>=0.17.2->dadmatools) (5.9.4)\n",
            "Collecting pybcj>=0.6.0\n",
            "  Using cached pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "Collecting inflate64>=0.3.1\n",
            "  Using cached inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "Collecting brotli>=1.0.9\n",
            "  Using cached Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "Collecting regex\n",
            "  Using cached regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "Collecting boto3\n",
            "  Using cached boto3-1.26.114-py3-none-any.whl (135 kB)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
            "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
            "Collecting thinc<8.2.0,>=8.1.8\n",
            "  Using cached thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.0.0->dadmatools) (23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Using cached pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Collecting pathy>=0.10.0\n",
            "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Using cached cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Using cached murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3.0.0->dadmatools) (67.6.1)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Using cached srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/557.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:06:36\u001b[0m"
          ]
        }
      ],
      "source": [
        "try: \n",
        "    import sklearn\n",
        "except:\n",
        "    %pip install -U scikit-learn numpy\n",
        "    \n",
        "try:\n",
        "    import hazm\n",
        "except:\n",
        "    %pip install hazm\n",
        "    \n",
        "try:\n",
        "    import dadmatools\n",
        "except:\n",
        "    %pip install dadmatools\n",
        "\n",
        "try:\n",
        "    import fasttext\n",
        "except:\n",
        "    %pip install fasttext\n",
        "\n",
        "try: \n",
        "    import wapiti\n",
        "except: \n",
        "    %pip install wapiti\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yku9-tEzGxwP"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "## **هم‌خوان‌سازی کد با کتاب‌خانه parsi-io**\n",
        "در پیاده‌سازی کد این تمرین،‌تلاش شده‌ که شیوه پیاده‌سازی سرویس، با الگوی پیاده‌سازی ابزار کتاب‌خانه parsi-io همگام باشد. \n",
        "`HeKasraExtractor`، کلاس\n",
        " اصلی این سرویس است که تلاش شده با چارچوبی متناسب با کتاب‌خانه یادشده توسعه داده شود. البته به علت رسیدن به ددلاین آپلود تمرین، فرصت برای پول ریکوئست به ریپازیتوری این کتاب‌خانه مهیا نشد. با تکمیل رعایت گایدلاین‌ها و قواعد کانتریبیوشن به ریپازیتوری پروژه یادشده، تلاش خواهیم کرد در آینده نزدیک به ریپازیتوری پارسی‌آی‌او پول‌ریکوئست دهیم. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJeC323GxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "# **یافتن و تصحیح ایرادات هه کسره**\n",
        "<br>\n",
        "\n",
        "در نوشتار فارسی، خطای ه‌کسره با الگوهای خاصی رخ می‌دهد. به طور کلی،‌این الگوها تا حد زیادی قاعده‌مند می‌باشند و با وجود استثنائات، می‌توان خطاهای «ه‌کسره» را به چند دسته خاص تقسیم کرد.  \n",
        "در این [بلاگ‌پست](https://blog.irandargah.com/غلط‌های-نگارشی-و-املایی،-قاتل-اعتبار‎/) \n",
        "به دسته‌بندی‌های مختلف خطای هکسره در زبان فارسی اشاره مختصر و مفیدی شده‌است. در این تمرین نیز پیاده‌سازی تشخیص هکسره را بر مبنای منابع از این دست انجام داده‌ایم. در ادامه به طور مختصر الگوهای رایج این خطای املایی شرح داده می‌شوند. \n",
        "<h4>\n",
        "    بیان روش کلی\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QEZx7BEjGxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h3>\n",
        "    پیاده‌سازی کلاس HeKasraExtractor\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUJIKyhzGxwQ"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class HeKasraCorrection: \n",
        "    def __init__(self, processed_text):\n",
        "        self.processed_text = processed_text\n",
        "        self.corrections = {\n",
        "            'correct': processed_text['raw_text'],\n",
        "        }\n",
        "        self.correction_judgements = defaultdict(list)\n",
        "    \n",
        "    def vote_for_correction(self, invalid_token, corrected_token, str_index, order=10):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': invalid_token,\n",
        "            'corrected_token': corrected_token,\n",
        "            'str_index': str_index,\n",
        "            'order': order,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        "    \n",
        "    def veto_correction(self, already_correct_token, str_index):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': already_correct_token,\n",
        "            'corrected_token': already_correct_token,\n",
        "            'str_index': str_index,\n",
        "            'order': 0,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        " \n",
        "    def apply_correction_judgements(self, token, str_index):\n",
        "        # print('applying', token, str_index, self.correction_judgements)\n",
        "        judgements = self.correction_judgements[str_index]\n",
        "        # print('and now judgements', self.correction_judgements, self.correction_judgements.keys(), token, str_index)\n",
        "        if len(judgements) == 0:\n",
        "            return\n",
        "        \n",
        "        # print('judgements for token', token, str_index, judgements)\n",
        "        sorted_judgements = sorted(judgements, key=lambda x: x['order'])\n",
        "        prioritized_correction = sorted_judgements[0]\n",
        "        corrected_form = self.corrections['correct'][:str_index] + prioritized_correction['corrected_token'] + self.corrections['correct'][str_index+len(prioritized_correction['invalid_token']):]\n",
        "        self.corrections['correct'] = corrected_form\n",
        "        if token != prioritized_correction['corrected_token']:\n",
        "            self.corrections[prioritized_correction['invalid_token']] = [int(str_index), int(str_index)+len(prioritized_correction['invalid_token'])] \n",
        "              \n",
        "    def finalize(self):\n",
        "        for str_index in self.correction_judgements.copy().keys():\n",
        "            self.apply_correction_judgements(self.correction_judgements['invalid_token'], str_index)\n",
        "        if self.corrections['correct'] == self.processed_text['raw_text']:\n",
        "          self.corrections = {}\n",
        "        return {\n",
        "            **self.processed_text,\n",
        "            'correction': self.corrections,\n",
        "        }       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nUBayxdKNFe"
      },
      "outputs": [],
      "source": [
        "from dadmatools.embeddings import get_embedding\n",
        "# Some downloading, so separate the cell\n",
        "embeddings = get_embedding('glove-wiki')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK3TWkIUGxwQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from hazm import WordTokenizer, POSTagger, Normalizer, Lemmatizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "tokenizer = WordTokenizer()\n",
        "tagger = POSTagger(model=\"model/postagger.model\")\n",
        "\n",
        "class HeKasraExtractor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        normalized_text = normalizer.normalize(text)\n",
        "        tokens = tokenizer.tokenize(normalized_text)\n",
        "        tagged_tokens = tagger.tag(tokens)\n",
        "\n",
        "        return {\n",
        "            'raw_text': text,\n",
        "            'normalized_text': normalized_text,\n",
        "            'tokens': tokens,\n",
        "            'pos_tags': tagged_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "    def vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "\n",
        "        # he_kasra_pattern = re.compile(r\"^[^ه]+[ِ]\\b\")\n",
        "\n",
        "        pos_pairs = zip(processed_text['pos_tags'][:-1], processed_text['pos_tags'][1:])\n",
        "\n",
        "        for ppair in pos_pairs:\n",
        "          p1, p2 = ppair\n",
        "          token_1, tag_1 = p1\n",
        "          token_2, tag_2 = p2\n",
        "          first_token_roles = ('N', 'Ne', 'PRO', 'AJ')\n",
        "          second_token_roles = ('N', 'Ne', 'AJ', 'PRO')\n",
        "          if tag_1 in first_token_roles and tag_2 in second_token_roles:\n",
        "              if re.match(he_kasra_pattern, token_1):\n",
        "                he_kasra_correction.vote_for_correction(token_1, token_1[:-1], processed_text['raw_text'].index(token_1))\n",
        "\n",
        "    def check_word_contains_he(self, word):\n",
        "        \n",
        "        normalized_word = normalizer.normalize(word)\n",
        "        if not normalized_word.endswith('ه'):\n",
        "            pass\n",
        "        \n",
        "        word_without_he = normalized_word[:-1]\n",
        "        try:\n",
        "          similarity = embeddings.similarity(word_without_he, normalized_word)\n",
        "          # Some arbitrary threshold, cause we're not a bunch of data scientists doing data science here. \n",
        "          return similarity > 0.8\n",
        "\n",
        "        except KeyError:\n",
        "          return False\n",
        "    \n",
        "    def veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "        for token, tag in processed_text['pos_tags']:\n",
        "            contains_he = self.check_word_contains_he(token)\n",
        "            if contains_he:\n",
        "                he_kasra_correction.veto_correction(token, processed_text['raw_text'].index(token))\n",
        "                                \n",
        "                                \n",
        "    def run(self, input_sentence):\n",
        "        prep_text = self.preprocess(input_sentence)\n",
        "        he_kasra_correction = HeKasraCorrection(prep_text)\n",
        "        \n",
        "        pipe = [\n",
        "            self.vote_n_adj_he_kasra, \n",
        "            self.veto_if_word_he_part_of_word,\n",
        "        ]\n",
        "        for func in pipe:\n",
        "            func(he_kasra_correction, prep_text)\n",
        "            \n",
        "        result = he_kasra_correction.finalize()\n",
        "        # print(result)\n",
        "        return result['correction']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgM5R2UYGxwR",
        "outputId": "fafba004-cd29-4fc5-c00b-fececa553381"
      },
      "outputs": [],
      "source": [
        "hkasra_extractor = HeKasraExtractor()\n",
        "input_samples = ['کوروشه کبیر', 'حال من خوب است.', 'حاله من خوبه', 'من اگه کتابه تو رو داشتم', 'این دختره دیوانه', 'گل زیبا', 'گله زیبایی را تقدیم کردم']\n",
        "\n",
        "for sample in input_samples:\n",
        "  res = hkasra_extractor.run(sample)\n",
        "  print(res.items())\n",
        "  # print(hkasra_extractor.run(sample))\n",
        "    # print(hkasra_extractor.preprocess(sample))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UESv4uYkGxwR"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h3>\n",
        "    ارزیابی ماژول\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3VsvYuXeGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h4>\n",
        "    خطای ه اضافه بجای نقش‌نمای اضافه\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJ0NRj3GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zHMbFb-tGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h4>\n",
        "    خطای عدم جای‌گذاری هکسره\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufkaPbX5GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPLnVLaGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h3>\n",
        "    تست عملکرد ماژول با تست‌های اتوماتیک\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBEJYEIIGxwT"
      },
      "outputs": [],
      "source": [
        "class HeHasraTests:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXxw3ofQGxwQ"
      },
      "outputs": [],
      "source": [
        "vowels = {'ا', 'و', 'ی', 'آ', 'اُ', 'اِ', 'اَ'}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
