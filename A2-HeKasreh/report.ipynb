{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mYE-vgHGxwL"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
        "<h3><center>تمرین دوم درس پردازش زبان‌های طبیعی</center></h3>\n",
        "<h4><center>چالش درست‌کردن هه کسره (گروه چهار)</center></h4>\n",
        "<table width='100%' style=\"border: none;\">\n",
        "    <tr style=\"border: none; text-align: center;\">\n",
        "        <td style=\"border: none;\"><h5>علیرضا بلال</h5></td>\n",
        "        <td style=\"border: none;\"><h5>زهرا رجالی</h5></td>\n",
        "        <td style=\"border: none;\"><h5>جواد راضی</h5></td>\n",
        "    </tr>\n",
        "</table>\n",
        "<h5 style=\"font-size: 16px;\"><center> ترم ۴۰۱۲ </center></h5>\n",
        "<br/>\n",
        "<hr/>\n",
        "<br/>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar', 'B Lotus', 'Calibri'\" size=3><div dir='rtl' align='justify'>\n",
        "<b>\n",
        "    فایل ژوپیتر این تمرین در کولب توسعه داده و تست شده‌است. ابزارهایی نظیر دادماتولز، به تجربه ما، در بعضی از محیط‌های محلی به خاطر یک سری تداخلات با نسخه پکیج‌ها در نصب به مشکل می‌خورند. اما این فایل هم در محیط کولب، هم با ایمیج داکر jupyter/datascience-notebook تست شده‌است و همه قطعه‌کدها خروجی مورد انتظار را می‌دهند. اگه در بازتولید خروجی بعضی سل‌ها مشکلی وجود داشت، ممنون می‌شویم در صورت امکان به ما اطلاع دهید تا فایل را در محیطی که قابل اجرا است، اجرا نموده و خروجی را نمایش دهیم. . \n",
        "</b>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6RAmvwTvGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "#    **خطای هه کسره**\n",
        "در نوشتار فارسی، خطای \"ه‌کسره\" هنگامی به وجود می‌آید  که نشان کسره به درستی استفاده نشود.\n",
        "با اینکه صدای \"e\" در زبان فارسی دارای چندین نوع تکواژ است، اما برای نمایش آن در نوشتار دو نماد تکواژی وجود دارد. در مواقعی که به جای کسره (ـــِ) از \"ه/ـه\" استفاده شود یا برعکس، خطای گرامری هکسره به وجود می‌آید. در این تمرین، سرویسی را پیاده‌سازی کرده‌ایم که با دریافت یک متن فارسی، خطاهای «ه‌هکسره» آن را تشخیص داده و متن تصحیح شده را در پاسخ بر می‌گرداند. در ادامه گزارش، جزئیات پیاده‌سازی تمرین، و شیوه بکاررفته برای تشخیص خطای ه‌کسره شرح داده شده‌است. \n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CzBffAvSGxwO"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "### **نصب پکیج‌ها و ابزارهای مورد نیاز**\n",
        "\n",
        "کتاب‌خانه‌های اصلی مورد استفاده در این تمرین، کتاب‌خانه‌های هضم و دادماتولز بوده‌اند. کتاب‌خانه هضم برای POS Tagging و دادماتولز برای بررسی شباهت کلمات استفاده شده‌است. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ECGenxGxwO",
        "outputId": "dbfc1b33-2dae-4b2a-f8d5-dbe7cdc55f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dadmatools\n",
            "  Using cached dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
            "Collecting gensim>=3.6.0\n",
            "  Using cached gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
            "Collecting folium>=0.2.1\n",
            "  Using cached folium-0.14.0-py2.py3-none-any.whl (102 kB)\n",
            "Collecting html2text\n",
            "  Using cached html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting tabulate>=0.8.6\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "  Using cached pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "Collecting segtok>=1.5.7\n",
            "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting gdown>=4.3.1\n",
            "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Collecting hyperopt>=0.2.5\n",
            "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting Deprecated==1.2.6\n",
            "  Using cached Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting pyconll>=3.1.0\n",
            "  Using cached pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting py7zr>=0.17.2\n",
            "  Using cached py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "Collecting transformers>=4.9.1\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "Collecting bpemb>=0.3.3\n",
            "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu\n",
            "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from dadmatools) (3.3)\n",
            "Collecting sklearn>=0.0\n",
            "  Using cached sklearn-0.0.post4.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting spacy>=3.0.0\n",
            "  Using cached spacy-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "Collecting NERDA\n",
            "  Using cached NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting supar==1.1.2\n",
            "  Using cached supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "Requirement already satisfied: h5py>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dadmatools) (3.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "Collecting torch>=1.7.1\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Collecting wrapt<2,>=1.10\n",
            "  Using cached wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "Collecting stanza\n",
            "  Using cached stanza-1.5.0-py3-none-any.whl (802 kB)\n",
            "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (1.23.5)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bpemb>=0.3.3->dadmatools) (2.28.2)\n",
            "Collecting branca>=0.6.0\n",
            "  Using cached branca-0.6.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.10/site-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.11.0-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.3.1->dadmatools) (4.12.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "Collecting future\n",
            "  Using cached future-0.18.3.tar.gz (840 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.10/site-packages (from hyperopt>=0.2.5->dadmatools) (3.1)\n",
            "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Collecting py4j\n",
            "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Using cached pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Collecting inflate64>=0.3.1\n",
            "  Using cached inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "Collecting brotli>=1.0.9\n",
            "  Using cached Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "Collecting pybcj>=0.6.0\n",
            "  Using cached pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "Collecting pyzstd>=0.14.4\n",
            "  Using cached pyzstd-0.15.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\n",
            "Collecting texttable\n",
            "  Using cached texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Using cached pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr>=0.17.2->dadmatools) (5.9.4)\n",
            "Collecting multivolumefile>=0.2.3\n",
            "  Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting boto3\n",
            "  Using cached boto3-1.26.114-py3-none-any.whl (135 kB)\n",
            "Collecting regex\n",
            "  Using cached regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "Collecting sacremoses\n",
            "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting typer<0.8.0,>=0.3.0\n",
            "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting thinc<8.2.0,>=8.1.8\n",
            "  Using cached thinc-8.1.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (910 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Using cached wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Using cached pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3.0.0->dadmatools) (67.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.0.0->dadmatools) (23.0)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Using cached cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Using cached srsly-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Using cached murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.10.0\n",
            "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m692.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m381.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m123.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:13\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m354.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:14\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m283.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m933.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m00:27\u001b[0m7.1 MB\u001b[0m \u001b[31m496.1 kB/s\u001b[0m eta \u001b[36m0:18:18\u001b[0m�━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/557.1 MB\u001b[0m \u001b[31m424.9 kB/s\u001b[0m eta \u001b[36m0:21:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m�\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->dadmatools) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->dadmatools) (4.5.0)\n",
            "Collecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/177.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:01:39\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.1->dadmatools) (0.40.0)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.1.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m309.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting cmake\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m522.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m772.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m536.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from NERDA->dadmatools) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bpemb>=0.3.3->dadmatools) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bpemb>=0.3.3->dadmatools) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m912.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->dadmatools) (8.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.3.2.post1)\n",
            "Collecting botocore<1.30.0,>=1.29.114\n",
            "  Downloading botocore-1.29.114-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->NERDA->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NERDA->dadmatools) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NERDA->dadmatools) (2023.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->pytorch-transformers>=1.1.0->dadmatools) (1.2.0)\n",
            "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza->supar==1.1.2->dadmatools) (4.21.12)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m423.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->dadmatools) (1.3.0)\n",
            "Building wheels for collected packages: sklearn, future, progressbar, sacremoses, emoji, lit\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
            "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
            "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
            "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
            "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m More information is available at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m If the previous advice does not cover your use case, feel free to report it at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed building wheel for sklearn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for sklearn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
            "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
            "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
            "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
            "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m More information is available at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m If the previous advice does not cover your use case, feel free to report it at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for sklearn\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492022 sha256=58dfc5ffd21eaf56f51e51d155aed5074806917d9e11587158713b8ec92a272f\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/5e/a9/47/f118e66afd12240e4662752cc22cefae5d97275623aa8ef57d\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12067 sha256=81133fd2f797bec9299120c28675c771557be6ec764aded2836e41ba60ef80dc\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=af5ab9eb58a5777a0ea8f6187cb398d96eaab568a43cde28e5ec51864bd99831\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234911 sha256=24c8a73796e9c8d45cb44a232232d23051f805064fce7c72da9a4799ada065b1\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for lit: filename=lit-16.0.1-py3-none-any.whl size=88172 sha256=7a85e6d8ff565ca9c76c086289c6bfad09581d1c8ee02ba680b908e567af4050\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/33/c2/b7/b91592eb5167b4293827b97fb02d52686773dded13f7fb1054\n",
            "Successfully built future progressbar sacremoses emoji lit\n",
            "Failed to build sklearn\n",
            "Installing collected packages: tokenizers, tf-estimator-nightly, texttable, sklearn, sentencepiece, py4j, progressbar, lit, cymem, cmake, brotli, wrapt, wasabi, typer, tabulate, spacy-loggers, spacy-legacy, smart-open, regex, pyzstd, pyppmd, pydantic, pycryptodomex, pyconll, pybcj, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, murmurhash, multivolumefile, langcodes, jmespath, inflate64, html2text, future, filelock, emoji, conllu, catalogue, blis, srsly, segtok, sacremoses, py7zr, preshed, pathy, nvidia-cusolver-cu11, nvidia-cudnn-cu11, hyperopt, huggingface-hub, gensim, Deprecated, branca, botocore, transformers, s3transfer, gdown, folium, confection, bpemb, thinc, boto3, spacy, triton, torch, stanza, supar, pytorch-transformers, NERDA, dadmatools\n",
            "  Running setup.py install for sklearn ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for sklearn\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[18 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
            "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
            "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
            "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
            "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m More information is available at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m If the previous advice does not cover your use case, feel free to report it at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package/issues/new\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while trying to install package.\n",
            "\u001b[31m╰─>\u001b[0m sklearn\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n",
            "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "try: \n",
        "    import sklearn\n",
        "except:\n",
        "    %pip install -U scikit-learn numpy\n",
        "    \n",
        "try:\n",
        "    import hazm\n",
        "except:\n",
        "    %pip install hazm\n",
        "    \n",
        "try:\n",
        "    import dadmatools\n",
        "except:\n",
        "    %pip install dadmatools\n",
        "\n",
        "try:\n",
        "    import fasttext\n",
        "except:\n",
        "    %pip install fasttext\n",
        "\n",
        "try: \n",
        "    import wapiti\n",
        "except: \n",
        "    %pip install wapiti\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yku9-tEzGxwP"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "## **هم‌خوان‌سازی کد با کتاب‌خانه parsi-io**\n",
        "در پیاده‌سازی کد این تمرین،‌تلاش شده‌ که شیوه پیاده‌سازی سرویس، با الگوی پیاده‌سازی ابزار کتاب‌خانه parsi-io همگام باشد. \n",
        "`HeKasraExtractor`، کلاس\n",
        " اصلی این سرویس است که تلاش شده با چارچوبی متناسب با کتاب‌خانه یادشده توسعه داده شود. البته به علت رسیدن به ددلاین آپلود تمرین، فرصت برای پول ریکوئست به ریپازیتوری این کتاب‌خانه مهیا نشد. با تکمیل رعایت گایدلاین‌ها و قواعد کانتریبیوشن به ریپازیتوری پروژه یادشده، تلاش خواهیم کرد در آینده نزدیک به ریپازیتوری پارسی‌آی‌او پول‌ریکوئست دهیم. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJeC323GxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "# **یافتن و تصحیح ایرادات هه کسره**\n",
        "<br>\n",
        "\n",
        "در نوشتار فارسی، خطای ه‌کسره با الگوهای خاصی رخ می‌دهد. به طور کلی،‌این الگوها تا حد زیادی قاعده‌مند می‌باشند و با وجود استثنائات، می‌توان خطاهای «ه‌کسره» را به چند دسته خاص تقسیم کرد.  \n",
        "در این [بلاگ‌پست](https://blog.irandargah.com/%D8%BA%D9%84%D8%B7%E2%80%8C%D9%87%D8%A7%DB%8C-%D9%86%DA%AF%D8%A7%D8%B1%D8%B4%DB%8C-%D9%88-%D8%A7%D9%85%D9%84%D8%A7%DB%8C%DB%8C%D8%8C-%D9%82%D8%A7%D8%AA%D9%84-%D8%A7%D8%B9%D8%AA%D8%A8%D8%A7%D8%B1/) \n",
        "به دسته‌بندی‌های مختلف خطای هکسره در زبان فارسی اشاره مختصر و مفیدی شده‌است. در این تمرین نیز پیاده‌سازی تشخیص هکسره را بر مبنای منابع از این دست انجام داده‌ایم. در ادامه به طور مختصر الگوهای رایج این خطای املایی شرح داده می‌شوند. \n",
        "\n",
        "#### الگوهای رایج غلط ه‌کسره\n",
        "1. **صفت و موصوف یا مضاف و مضافه‌الیه**  \n",
        "در ترکیبات وصفی و اضافی، باید بعد از موصوف یا مضاف الیه، از کسرهٔ اضافهٔ (ــِ) بین دو کلمهٔ مورد نظر استفاده کرد. بهتر است از \"ه\" یا \"ـه\" در این نوع ترکیبات استفاده نشود. \n",
        "    * **استثنا:‌ زمانی که تکواژ ه بخشی از واژه است**  \n",
        "    در موارد دیگری که ممکن است با خطاهای هکسره در فارسی مواجه شویم، حالتی وجود دارد که \"ه/ ـه\" به عنوان حرف اصلی در انتهای واژه ظاهر شده و جزئی از واژه است، و به این نوع از \"ه\" هم \"ه غیر ملفوظ\" گفته می‌شود. در این حالت‌ها، \"ه/ ـه\" نباید حذف شود و استفاده از کسره بجای آن مناسب نیست. \n",
        "<br><br>\n",
        "\n",
        "2. **تکواژ ه به عنوان معرفه‌ساز**  \n",
        "معرفه استفاده می‌شود. اسم‌های معرفه، اسامی هستند که برای شناسایی کسی یا چیزی به کار می‌روند. در صورتی که نویسنده در متن خود می‌خواهد به کسی یا چیزی اشاره کند که او را می‌شناسد، تکواژ \"ه\" به آخر کلمات اضافه می‌شود. استفاده از هکسره (-ِ) در این موارد کاملاً نادرست است. \n",
        "\n",
        "3. **تکواژ ه به جای فعل**  \n",
        "در زبان محاوره‌ای ما در برخی موارد به جای فعل \"است\" یا \"هست\"، از صدای \"e\" استفاده می‌کنیم. در این حالت‌ها باید از حرف \"ه\" به جای کسره استفاده کرد.\n",
        "همچنین در زبان محاوره، برای سوم شخص فعل‌ها، گاهی به جای پایان با \"ـَد\" از صدای \"e\" استفاده می‌شود. در چنین مواقعی نیز باید از حرف \"ه\" به جای کسره استفاده کرد.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QEZx7BEjGxwQ"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "## **پیاده‌سازی سامانه تشخیص خطای هکسره**\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUJIKyhzGxwQ"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class HeKasraCorrection: \n",
        "    def __init__(self, processed_text):\n",
        "        self.processed_text = processed_text\n",
        "        self.corrections = {\n",
        "            'correct': processed_text['raw_text'],\n",
        "        }\n",
        "        self.correction_judgements = defaultdict(list)\n",
        "    \n",
        "    def vote_for_correction(self, invalid_token, corrected_token, str_index, order=10):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': invalid_token,\n",
        "            'corrected_token': corrected_token,\n",
        "            'str_index': str_index,\n",
        "            'order': order,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        "    \n",
        "    def veto_correction(self, already_correct_token, str_index):\n",
        "        self.correction_judgements[str_index].append({\n",
        "            'invalid_token': already_correct_token,\n",
        "            'corrected_token': already_correct_token,\n",
        "            'str_index': str_index,\n",
        "            'order': 0,\n",
        "        })\n",
        "        return self.correction_judgements[str_index]\n",
        " \n",
        "    def apply_correction_judgements(self, token, str_index):\n",
        "        # print('applying', token, str_index, self.correction_judgements)\n",
        "        judgements = self.correction_judgements[str_index]\n",
        "        # print('and now judgements', self.correction_judgements, self.correction_judgements.keys(), token, str_index)\n",
        "        if len(judgements) == 0:\n",
        "            return\n",
        "        \n",
        "        # print('judgements for token', token, str_index, judgements)\n",
        "        sorted_judgements = sorted(judgements, key=lambda x: x['order'])\n",
        "        prioritized_correction = sorted_judgements[0]\n",
        "        corrected_form = self.corrections['correct'][:str_index] + prioritized_correction['corrected_token'] + self.corrections['correct'][str_index+len(prioritized_correction['invalid_token']):]\n",
        "        self.corrections['correct'] = corrected_form\n",
        "        if token != prioritized_correction['corrected_token']:\n",
        "            self.corrections[prioritized_correction['invalid_token']] = [int(str_index), int(str_index)+len(prioritized_correction['invalid_token'])] \n",
        "              \n",
        "    def finalize(self):\n",
        "        for str_index in self.correction_judgements.copy().keys():\n",
        "            self.apply_correction_judgements(self.correction_judgements['invalid_token'], str_index)\n",
        "        if self.corrections['correct'] == self.processed_text['raw_text']:\n",
        "          self.corrections = {}\n",
        "        return {\n",
        "            **self.processed_text,\n",
        "            'correction': self.corrections,\n",
        "        }       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nUBayxdKNFe"
      },
      "outputs": [],
      "source": [
        "from dadmatools.embeddings import get_embedding\n",
        "# Some downloading, so separate the cell\n",
        "embeddings = get_embedding('glove-wiki')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK3TWkIUGxwQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from hazm import WordTokenizer, POSTagger, Normalizer, Lemmatizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "tokenizer = WordTokenizer()\n",
        "tagger = POSTagger(model=\"model/postagger.model\")\n",
        "\n",
        "class HeKasraExtractor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        normalized_text = normalizer.normalize(text)\n",
        "        tokens = tokenizer.tokenize(normalized_text)\n",
        "        tagged_tokens = tagger.tag(tokens)\n",
        "\n",
        "        return {\n",
        "            'raw_text': text,\n",
        "            'normalized_text': normalized_text,\n",
        "            'tokens': tokens,\n",
        "            'pos_tags': tagged_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "    def vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "\n",
        "        # he_kasra_pattern = re.compile(r\"^[^ه]+[ِ]\\b\")\n",
        "\n",
        "        pos_pairs = zip(processed_text['pos_tags'][:-1], processed_text['pos_tags'][1:])\n",
        "\n",
        "        for ppair in pos_pairs:\n",
        "          p1, p2 = ppair\n",
        "          token_1, tag_1 = p1\n",
        "          token_2, tag_2 = p2\n",
        "          first_token_roles = ('N', 'Ne', 'PRO', 'AJ')\n",
        "          second_token_roles = ('N', 'Ne', 'AJ', 'PRO')\n",
        "          if tag_1 in first_token_roles and tag_2 in second_token_roles:\n",
        "              if re.match(he_kasra_pattern, token_1):\n",
        "                he_kasra_correction.vote_for_correction(token_1, token_1[:-1], processed_text['raw_text'].index(token_1))\n",
        "\n",
        "    def check_word_contains_he(self, word):\n",
        "        \n",
        "        normalized_word = normalizer.normalize(word)\n",
        "        if not normalized_word.endswith('ه'):\n",
        "            pass\n",
        "        \n",
        "        word_without_he = normalized_word[:-1]\n",
        "        try:\n",
        "          similarity = embeddings.similarity(word_without_he, normalized_word)\n",
        "          # Some arbitrary threshold, cause we're not a bunch of data scientists doing data science here. \n",
        "          return similarity > 0.8\n",
        "\n",
        "        except KeyError:\n",
        "          return False\n",
        "    \n",
        "    def veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):\n",
        "        he_kasra_pattern = r'\\b\\w*ه\\b'\n",
        "        for token, tag in processed_text['pos_tags']:\n",
        "            contains_he = self.check_word_contains_he(token)\n",
        "            if contains_he:\n",
        "                he_kasra_correction.veto_correction(token, processed_text['raw_text'].index(token))\n",
        "                                \n",
        "                                \n",
        "    def run(self, input_sentence):\n",
        "        prep_text = self.preprocess(input_sentence)\n",
        "        he_kasra_correction = HeKasraCorrection(prep_text)\n",
        "        \n",
        "        pipe = [\n",
        "            self.vote_n_adj_he_kasra, \n",
        "            self.veto_if_word_he_part_of_word,\n",
        "        ]\n",
        "        for func in pipe:\n",
        "            func(he_kasra_correction, prep_text)\n",
        "            \n",
        "        result = he_kasra_correction.finalize()\n",
        "        # print(result)\n",
        "        return result['correction']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgM5R2UYGxwR",
        "outputId": "fafba004-cd29-4fc5-c00b-fececa553381"
      },
      "outputs": [],
      "source": [
        "hkasra_extractor = HeKasraExtractor()\n",
        "input_samples = ['کوروشه کبیر', 'حال من خوب است.', 'حاله من خوبه', 'من اگه کتابه تو رو داشتم', 'این دختره دیوانه', 'گل زیبا', 'گله زیبایی را تقدیم کردم']\n",
        "\n",
        "for sample in input_samples:\n",
        "  res = hkasra_extractor.run(sample)\n",
        "  print(res.items())\n",
        "  # print(hkasra_extractor.run(sample))\n",
        "    # print(hkasra_extractor.preprocess(sample))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UESv4uYkGxwR"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h3>\n",
        "    ارزیابی ماژول\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3VsvYuXeGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h4>\n",
        "    خطای ه اضافه بجای نقش‌نمای اضافه\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxJ0NRj3GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zHMbFb-tGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h4>\n",
        "    خطای عدم جای‌گذاری هکسره\n",
        "</h4>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufkaPbX5GxwS"
      },
      "outputs": [],
      "source": [
        "pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4zPLnVLaGxwS"
      },
      "source": [
        "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir='rtl' align='justify'>\n",
        "<h3>\n",
        "    تست عملکرد ماژول با تست‌های اتوماتیک\n",
        "</h3>\n",
        "<div>\n",
        "    توضیحات\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBEJYEIIGxwT"
      },
      "outputs": [],
      "source": [
        "class HeHasraTests:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXxw3ofQGxwQ"
      },
      "outputs": [],
      "source": [
        "vowels = {'ا', 'و', 'ی', 'آ', 'اُ', 'اِ', 'اَ'}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
