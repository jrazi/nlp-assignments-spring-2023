{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3><center>تمرین دوم درس پردازش زبان‌های طبیعی</center></h3>\n",
    "<h4><center>چالش درست‌کردن هه کسره (گروه چهار)</center></h4>\n",
    "<table width='100%' style=\"border: none;\">\n",
    "    <tr style=\"border: none; text-align: center;\">\n",
    "        <td style=\"border: none;\"><h5>علیرضا بلال</h5></td>\n",
    "        <td style=\"border: none;\"><h5>زهرا رجالی</h5></td>\n",
    "        <td style=\"border: none;\"><h5>جواد راضی</h5></td>\n",
    "    </tr>\n",
    "</table>\n",
    "<h5 style=\"font-size: 16px;\"><center> ترم ۴۰۱۲ </center></h5>\n",
    "<br/>\n",
    "<hr/>\n",
    "<br/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    خلاصه کارهای صورت‌گرفته\n",
    "</h3>\n",
    "<span>\n",
    "    در این تمرین، ماژولی را توسعه داده‌ایم که با دریافت یک متن فارسی به عنوان ورودی، ایرادات موسوم به «هکسره» آن را یافته، رفع کرده و متنی بدون خطاهای هکسره را برمی‌گرداند. \n",
    "</span>\n",
    "<br/>\n",
    "<span>\n",
    "    {بیان ایده برای نحوه یافتن غلط هکسره}\n",
    "</span>\n",
    "<span>\n",
    "    {ابزارها و پکیج‌های استفاده شده}\n",
    "</span>\n",
    "<span>\n",
    "    {نتایج دستیابی‌شده در تمرین}\n",
    "</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    خطای هه کسره\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات کلی در مورد خطای هه کسره و قواعد تشخیصش \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    نصب پکیج‌ها و ابزارهای مورد نیاز\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dadmatools\n",
      "  Using cached dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
      "Collecting bpemb>=0.3.3\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting folium>=0.2.1\n",
      "  Using cached folium-0.14.0-py2.py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: h5py>=3.3.0 in c:\\users\\javad\\anaconda3\\lib\\site-packages (from dadmatools) (3.7.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\javad\\anaconda3\\lib\\site-packages (from dadmatools) (3.3)\n",
      "Collecting pytorch-transformers>=1.1.0\n",
      "  Using cached pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting torch>=1.7.1\n",
      "  Downloading torch-2.0.0-cp39-cp39-win_amd64.whl (172.3 MB)\n",
      "     -------------------------------------- 172.3/172.3 MB 1.0 MB/s eta 0:00:00\n",
      "Collecting hyperopt>=0.2.5\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting transformers>=4.9.1\n",
      "  Using cached transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "Collecting Deprecated==1.2.6\n",
      "  Using cached Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
      "Collecting spacy>=3.0.0\n",
      "  Downloading spacy-3.5.2-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting pyconll>=3.1.0\n",
      "  Using cached pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Collecting py7zr>=0.17.2\n",
      "  Using cached py7zr-0.20.4-py3-none-any.whl (66 kB)\n",
      "Collecting gdown>=4.3.1\n",
      "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting sklearn>=0.0\n",
      "  Using cached sklearn-0.0.post4.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [8 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Javad\\AppData\\Local\\Temp\\pip-install-okc0nqg8\\sklearn_e58d3d10bbc444eda5d95be784ed7f3f\\setup.py\", line 10, in <module>\n",
      "      LONG_DESCRIPTION = f.read()\n",
      "    File \"C:\\Users\\Javad\\anaconda3\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "      return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "  UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 7: character maps to <undefined>\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wapiti\n",
      "  Using cached wapiti-0.1.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Javad\\AppData\\Local\\Temp\\pip-install-97kj1ha2\\wapiti_fb08f7910ccc4a8ba5cac201d029981d\\setup.py\", line 28, in <module>\n",
      "      raise NotImplementedError(\"wapiti Python 3 support en route to your location\")\n",
      "  NotImplementedError: wapiti Python 3 support en route to your location\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import hazm\n",
    "except:\n",
    "    %pip install hazm\n",
    "    \n",
    "try:\n",
    "    import dadmatools\n",
    "except:\n",
    "    %pip install dadmatools\n",
    "\n",
    "try: \n",
    "    import wapiti\n",
    "except: \n",
    "    %pip install wapiti"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    هم‌خوان‌سازی کد با کتاب‌خانه parsi-io\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    یافتن و تصحیح ایرادات هه کسره\n",
    "</h3>\n",
    "<div>\n",
    "</div>\n",
    "\n",
    "<h4>\n",
    "    بیان روش کلی\n",
    "</h4>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    پیاده‌سازی کلاس HeKasraExtractor\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels = {'ا', 'و', 'ی', 'آ', 'اُ', 'اِ', 'اَ'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class HeKasraCorrection: \n",
    "    def __init__(processed_text):\n",
    "        self.processed_text = processed_text\n",
    "        self.corrections = {\n",
    "            'correct': processed_text['raw_text'],\n",
    "        }\n",
    "        self.correction_judgements = defaultdict(list)\n",
    "    \n",
    "    def vote_for_correction(self, invalid_token, corrected_token, str_index, order=10):\n",
    "        self.correction_judgements[(str_index, invalid_token)] += {\n",
    "            'invalid_token': invalid_token,\n",
    "            'corrected_token': corrected_token,\n",
    "            'str_index': str_index,\n",
    "            'order': order,\n",
    "        }\n",
    "        return self.correction_judgements[(str_index, invalid_token)]\n",
    "    \n",
    "    def veto_correction(self, already_correct_token, str_index):\n",
    "        self.correction_judgements[(str_index, already_correct_token)] += {\n",
    "            'invalid_token': already_correct_token,\n",
    "            'corrected_token': already_correct_token,\n",
    "            'str_index': str_index,\n",
    "            'order': 0,\n",
    "        }\n",
    "        return self.correction_judgements[(str_index, already_correct_token)]\n",
    " \n",
    "    def apply_correction_judgements(self, token, str_index):\n",
    "        judgements = self.correction_judgements[(str_index, token)]\n",
    "        if len(judgements) == 0:\n",
    "            return\n",
    "        \n",
    "        sorted_judgements = sorted(judgements, key=lambda x: x['order'])\n",
    "        prioritized_correction = sorted_judgements[0]\n",
    "        corrected_form = self.corrections['correct'][:str_index] + prioritized_correction['corrected_token'] + self.corrections['correct'][str_index+len(prioritized_correction['invalid_token']):]\n",
    "        self.corrections['correct'] = corrected_form\n",
    "        if token != prioritized_correction['corrected_token']:\n",
    "            self.corrections[prioritized_correction['invalid_token']] = [str_index, str_index+len(prioritized_correction['invalid_token'])] \n",
    "              \n",
    "    def finalize(self):\n",
    "        for token, str_index in self.correction_judgements.keys():\n",
    "            self.apply_correction_judgements(token, str_index)\n",
    "        return {\n",
    "            **self.processed_text,\n",
    "            'correction': self.corrections,\n",
    "        }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (1701411982.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Javad\\AppData\\Local\\Temp\\ipykernel_15308\\1701411982.py\"\u001b[1;36m, line \u001b[1;32m30\u001b[0m\n\u001b[1;33m    for token, tag in processed_text['pos_tags']]:\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from hazm import WordTokenizer, POSTagger, Normalizer, Lemmatizer\n",
    "from dadmatools.embeddings import GloVeEmbedding\n",
    "\n",
    "normalizer = Normalizer()\n",
    "tokenizer = WordTokenizer()\n",
    "tagger = POSTagger(model=\"model/postagger.model\")\n",
    "embeddings = GloVeEmbedding()\n",
    "\n",
    "class HeKasraExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        normalized_text = normalizer.normalize(text)\n",
    "        tokens = tokenizer.tokenize(normalized_text)\n",
    "        tagged_tokens = tagger.tag(tokens)\n",
    "\n",
    "        return {\n",
    "            'raw_text': text,\n",
    "            'normalized_text': normalized_text,\n",
    "            'tokens': tokens,\n",
    "            'pos_tags': tagged_tokens\n",
    "        }\n",
    "\n",
    "\n",
    "    def vote_n_adj_he_kasra(self, he_kasra_correction, processed_text):\n",
    "        he_kasra_pattern = re.compile(r\"^[^ه]+[ِ]\\b\")\n",
    "\n",
    "        for token, tag in processed_text['pos_tags']:\n",
    "            # Check if the tag is a noun or an adjective\n",
    "            if tag == 'N' or tag == 'AJ':\n",
    "                # Check if the token matches the He Kasra pattern\n",
    "                if he_kasra_pattern.match(token):\n",
    "                    he_kasra_correction.vote_for_correction(token, token[:-1]+'ه', processed_text['raw_text'].index(token))\n",
    "\n",
    "\n",
    "    def check_word_contains_he(self, word):\n",
    "        \n",
    "        normalized_word = normalizer.normalize(word)\n",
    "        if not normalized_word.endswith('ه'):\n",
    "            pass\n",
    "        \n",
    "        word_without_he = normalized_word[:-1]\n",
    "        word_embedding = embeddings.get_vector(normalized_word)\n",
    "        word_without_he_embedding = embeddings.get_vector(word_without_he)\n",
    "\n",
    "        # Check if the words are semantically related\n",
    "        similarity = embeddings.cosine_similarity(word_embedding, word_without_he_embedding)\n",
    "\n",
    "        # Some arbitrary threshold, cause we're not a bunch of data scientists doing data science here. \n",
    "        return similarity > 0.8\n",
    "    \n",
    "    def veto_if_word_he_part_of_word(self, he_kasra_correction, processed_text):\n",
    "        he_kasra_pattern = re.compile(r\"^[^ه]+[ِ]\\b\")\n",
    "        for token, tag in processed_text['pos_tags']:\n",
    "            contains_he = self.check_word_contains_he(token)\n",
    "            if contains_he:\n",
    "                he_kasra_correction.veto_correction(token, processed_text['raw_text'].index(token))\n",
    "                                \n",
    "                                \n",
    "    def run(self, input_sentence):\n",
    "        prep_text = self.preprocess(input_sentence)\n",
    "        he_kasra_correction = HeKasraCorrection(prep_text)\n",
    "        \n",
    "        pipe = [self.vote_n_adj_he_kasra, self.veto_if_word_he_part_of_word]\n",
    "        for func in pipe:\n",
    "            func(he_kasra_correction, prep_text)\n",
    "            \n",
    "        result = he_kasra_correction.finalize()\n",
    "        return result['correction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wapiti'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15308\\3152472472.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhkasra_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15308\\3138294469.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPOSTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model/postagger.model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         return {\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hazm\\SequenceTagger.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, patterns, **options)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[1;32mfrom\u001b[0m \u001b[0mwapiti\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wapiti'"
     ]
    }
   ],
   "source": [
    "hkasra_extractor = HeKasraExtractor()\n",
    "input_samples = ['کوروشه کبیر', 'حال من خوب است.', 'حاله من خوبه', 'من اگه کتابه تو رو داشتم', 'این دختره دیوانه', 'گل زیبا', 'گله زیبایی را تقدیم کردم']\n",
    "\n",
    "for sample in input_samples:\n",
    "    print(hkasra_extractor.preprocess(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    ارزیابی ماژول\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h4>\n",
    "    خطای ه اضافه بجای نقش‌نمای اضافه\n",
    "</h4>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h4>\n",
    "    خطای عدم جای‌گذاری هکسره\n",
    "</h4>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"'vazirmatn', 'Vazir', 'B Nazanin', 'XB Zar'\" size=4><div dir=rtl>\n",
    "<h3>\n",
    "    تست عملکرد ماژول با تست‌های اتوماتیک\n",
    "</h3>\n",
    "<div>\n",
    "    توضیحات\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeHasraTests:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
